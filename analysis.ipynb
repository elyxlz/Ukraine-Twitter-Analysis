{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230816d-b8c8-4091-abdc-093ebf6f7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import six\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import dill\n",
    "\n",
    "import hdbscan\n",
    "from umap import UMAP\n",
    "from umap.parametric_umap import ParametricUMAP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163ba36f-a214-4351-a719-4ea3f4441bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_languages = [\"und\", \"hr\",  \"cs\",  \"et\",  \"fi\",  \"fr\",  \"de\",  \"el\",  \"hu\",  \"it\",  \"lv\",  \"lt\",  \"pl\",  \"pt\",  \"ro\",  \"sk\",  \"sl\",  \"es\"]\n",
    "# Obtain dataframe\n",
    "csv_collection = []\n",
    "for dirname, _, filenames in os.walk('./Data/ukraine-russian-crisis-twitter-dataset-1-2-m-rows/'):\n",
    "    for filename in filenames:\n",
    "        fullpath= os.path.join(dirname, filename)\n",
    "        csv_collection.append(fullpath)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for i, v in enumerate(tqdm(csv_collection)):\n",
    "    #print(f\"{i+1} out of {len(csv_collection)}\")\n",
    "    #print(v)\n",
    "    tmp = pd.read_csv(v ,compression = 'gzip', index_col=0)[['tweetcreatedts', 'text','language']] # only keep important columns to lower memory consumption\n",
    "    tmp = tmp.drop_duplicates(subset=['text']) # remove duplicates as most are retweets\n",
    "    mask = tmp['language'].isin(desired_languages)\n",
    "    #print(f\"{len(tmp)} unique values in {i+1}th csv\")\n",
    "    tmp = tmp[mask]\n",
    "    df = pd.concat([df, tmp], axis=0)\n",
    "\n",
    "df.reset_index()\n",
    "df.to_pickle('./Pickles/raw_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91beb058-7242-4f36-a7cf-8099b4bfd209",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_pickle('./Pickles/raw_df.pkl')\n",
    "raw_df.head()\n",
    "raw_df['language'].unique(), len(raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39bc669-b7b6-4372-80a4-71922bb6521b",
   "metadata": {},
   "source": [
    "**Randomly sampling Part of Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc88ccb-29aa-46b7-8285-6b7e9183c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.random.choice(len(raw_df), int(np.ceil(0.75*len(raw_df))), replace=False)\n",
    "df_small = raw_df.iloc[idxs]\n",
    "df_small.to_pickle('./Pickles/df_small.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab92530-2df4-406e-9cfb-4cfe4b6b9d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = pd.read_pickle('./Pickles/df_small.pkl')\n",
    "df_small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a4891-6569-4dfe-9a78-84235579dc3c",
   "metadata": {},
   "source": [
    "**Plotting Language Distribution (todo: plotly)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2eace9-7be4-4756-a7a3-1ef49538a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "g=sns.barplot(x=df_small.language.value_counts()[:].index,y=df_small.language.value_counts()[:])\n",
    "g.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36a09a3-13f6-4e22-ba71-b5d52caacbc9",
   "metadata": {},
   "source": [
    "**Plotting Daily Tweet Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456c1486-acf0-42d3-a8ac-709943a6b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dailycounts = pd.to_datetime(df_small['tweetcreatedts']).dt.floor('d').value_counts().rename_axis('date').reset_index(name='count').sort_values(by='date')\n",
    "fig = px.line(df_dailycounts, x='date', y=\"count\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60c56fe-848f-43e6-a029-f0924fc43637",
   "metadata": {},
   "source": [
    "**Random Example of some unprocessed tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3413ebf9-29ac-4eb3-8d4d-9dc0d7685ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_small.iloc[np.random.choice(len(df_small), 5)]['text']: print(f\"{i}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373cb071-e505-42fd-ba34-412fe4e75361",
   "metadata": {},
   "source": [
    "**Preprocessing Data and Translating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24df5b80-0a3a-4c7b-ae5b-81df4ecb8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(examples):\n",
    "    text = examples['text']\n",
    "    new_text = []\n",
    "    text = re.sub('\\n', ' ', str(text)) # replace new lines with space\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t # replace tags with \"@user\"\n",
    "        t = 'http' if t.startswith(('http', 'www')) or t.endswith('.com') else t # replace links with \"http\"\n",
    "        new_text.append(t)\n",
    "    text = \" \".join(new_text)   \n",
    "    return dict(text = text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e437aa-d9a3-41d4-bf53-96aedde12cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = pd.DataFrame(Dataset.from_pandas(df_small).map(preprocess_tweet))\n",
    "df_proc.to_pickle('./Pickles/df_proc.pkl')\n",
    "df_proc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb5748-8265-4055-9c50-2fc25399db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proc = pd.read_pickle('./Pickles/df_proc.pkl')\n",
    "df_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d707f16-2eb4-4cab-aacc-e6120499b293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translating\n",
    "from google.cloud import translate_v2 as translate\n",
    "path = \"/home/elyx/ukraine-twitter-NLP/cred.json\" # path to google cloud api credentials json\n",
    "translate_client = translate.Client.from_service_account_json(path)\n",
    "\n",
    "def translate_text(examples):\n",
    "    \"\"\"Translates text into the target language\"\"\"\n",
    "    target = 'EN'\n",
    "    text = examples['text']\n",
    "    \n",
    "    result = translate_client.translate(text, target_language=target)\n",
    "    \n",
    "    return dict(text=[result[i][\"translatedText\"] for i in np.arange(len(result))],\n",
    "                language=[result[i]['detectedSourceLanguage'] for i in np.arange(len(result))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff252c-0231-4019-95e1-73168d19b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans = pd.DataFrame(Dataset.from_pandas(df_proc).map(translate_text, batched=True, batch_size=125))\n",
    "\n",
    "mask = df_trans['language'].isin(desired_languages)\n",
    "df_trans = df_trans[mask] # removing non desired languages that have been detected\n",
    "df_trans = df_trans.reset_index()\n",
    "df_trans['tweetcreatedts'] = pd.to_datetime(df_trans['tweetcreatedts']).dt.floor('d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b76c10-1783-4434-bff7-7ed4f2455ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing html codes\n",
    "def html_remover(examples):\n",
    "    text = examples['text']\n",
    "    text = re.sub('&quot;', '\"', str(text))\n",
    "    text = re.sub('&#39;', \"'\", str(text))\n",
    "    text = re.sub('&amp;', \"&\", str(text))\n",
    "    return dict(text = text)\n",
    "\n",
    "df_trans = pd.DataFrame(Dataset.from_pandas(df_trans).map(html_remover))\n",
    "df_trans.to_pickle('./Pickles/df_trans.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a909f8-23b7-487e-af68-5b50fff99d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans = pd.read_pickle('./Pickles/df_trans.pkl')\n",
    "df_trans.shape, df_trans.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5498a9-8bce-4a76-9a8e-f9e9d5cf9242",
   "metadata": {},
   "source": [
    "**Plotting new language distribution now that undetermined languages have been identified**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0c9f5-52e5-4702-a374-58763f5bab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_counts = df_trans.groupby('language').size().sort_values(ascending=False).reset_index().rename(columns={0:'count'})\n",
    "language_counts_mean = (df_trans.groupby('language').size() / 277).sort_values(ascending=False).reset_index().rename(columns={0:'count'})\n",
    "\n",
    "fig = px.bar(language_counts, x='language', y='count',\n",
    "             hover_data=['language', 'count'],\n",
    "             template='seaborn',\n",
    "             log_y=True,)\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            type = \"buttons\",\n",
    "            direction = \"left\",\n",
    "            buttons=list([\n",
    "                dict(\n",
    "                    args=[{\"y\": [language_counts['count']]}],\n",
    "                    label=\"Total\",\n",
    "                    method=\"update\"\n",
    "                ),\n",
    "                dict(\n",
    "                    args=[{\"y\": [language_counts_mean['count']]}],\n",
    "                    label=\"Mean\",\n",
    "                    method=\"update\"\n",
    "                )\n",
    "            ]),\n",
    "            pad={\"r\": 10, \"t\": 10},\n",
    "            showactive=True,\n",
    "            x=0.0,\n",
    "            xanchor=\"left\",\n",
    "            y=1.3,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"./Plots/language_count.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600c876a-dc05-493a-8574-5317038ab489",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Removing languages that have average with less than 30 tweets a day (CLT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1122c0-cc20-4d6f-bda8-0c71bcba387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_languages = list(language_counts_mean.iloc[0:10].language)\n",
    "mask = df_trans['language'].isin(final_languages)\n",
    "df_trans = df_trans[mask]\n",
    "df_trans.to_pickle('./Pickles/df_trans.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1898e9-d031-4653-afa5-5e8d5cd1e82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans = pd.read_pickle('./Pickles/df_trans.pkl')\n",
    "df_trans.shape, df_trans.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6074e71-cf8b-480f-8b40-7db765a75560",
   "metadata": {},
   "source": [
    "**Random Example of some processed tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f82e64-ff6d-4543-98f4-42931d4505bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df_trans.iloc[np.random.choice(len(df_trans), 5)]['text']: print(f\"{i}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611be8f-e3e5-407f-8a98-07226b5bdd40",
   "metadata": {},
   "source": [
    "**Plotting daily tweet count by language over time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde152be-b84c-4451-bf1f-2d2f99f2644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dailycounts = df_trans.groupby(['language', 'tweetcreatedts']).size().unstack('language').reset_index().fillna(0)\n",
    "df_dailycounts_norm = (df_dailycounts.iloc[:, 1:] / df_dailycounts.iloc[:, 1:].sum()).assign(tweetcreatedts=df_dailycounts['tweetcreatedts'])# divding every dayily count by the total number of tweets in that language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfdc292-3ae2-416f-a5f3-f8ed16bd6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "fig = go.Figure()\n",
    "fig = px.line(df_dailycounts, x='tweetcreatedts', y=df_dailycounts[final_languages].columns,\n",
    "              #hover_data={\"tweetcreatedts\": \"|%B %d, %Y\"},\n",
    "              template='seaborn',\n",
    "              title='Daily Tweets per language')\n",
    "fig.update_xaxes(\n",
    "    dtick=\"M1\",\n",
    "    tickformat=\"%b\\n%Y\")\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"./Plots/daily_tweets_per_language.html\") # add button to normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a029c73-3543-421a-bedb-4ca215ea09df",
   "metadata": {},
   "source": [
    "**Word Cloud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f50dc9-30cc-48e7-a275-2e3b3aed0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word cloud on all languages\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "stopwords_set = set(STOPWORDS)\n",
    "stopwords_set.update([\"user\", \"shows\", \"located\", \"dtype\", \"http\", 'text']) # adding extra stop words\n",
    "\n",
    "wordcloud = WordCloud(background_color='white',\n",
    "                     stopwords = stopwords_set,\n",
    "                      max_words = 300,\n",
    "                      max_font_size = 40,\n",
    "                      scale = 2,\n",
    "                      random_state=42\n",
    "                     ).generate(str(df_trans['text']))\n",
    "\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "plt.savefig('./Plots/wordcloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6d0b6-8775-4c5a-b794-fb5495b2204f",
   "metadata": {},
   "source": [
    "**Sample subset of clean tweets for semantic processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b3c6b9-ee68-4780-94b6-a90d1151d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 5000\n",
    "df_trans_subset = df_trans.iloc[np.random.choice(len(df_trans), sample_size)].reset_index(drop=True)\n",
    "df_trans_subset.to_pickle('./Pickles/df_trans_subset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c75f2dd-88df-4c62-b337-1a65fa9d2724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans_subset = pd.read_pickle('./Pickles/df_trans_subset.pkl')\n",
    "df_trans_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca456566-f28d-4f2c-aef7-6807256d8679",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans = df_trans_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c79af17-0245-4152-a105-0a8037175991",
   "metadata": {},
   "source": [
    "**Extracting semantic tweet features using pretrained transformers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b887668e-1130-497f-9d74-46dc4c9ba675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trans = pd.read_pickle('./Pickles/df_trans.pkl')\n",
    "df_trans.shape, df_trans.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c749d4-bd9f-4f1c-a1cd-fa6a071716c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device='cuda')#digio/Twitter4SSE\n",
    "# get features in 2 batches because kernel keeps dying\n",
    "def get_features(texts):\n",
    "    arr_features = model.encode(texts, show_progress_bar=True)\n",
    "    return arr_features\n",
    "#df_trans = df_trans.reset_index()\n",
    "length = len(df_trans.index)\n",
    "tweets_1 = df_trans.iloc[:int(length / 2)].text\n",
    "tweets_2 = df_trans.drop(tweets_1.index).text\n",
    "\n",
    "arr_features_1 = get_features(list(tweets_1))\n",
    "np.save('./Pickles/arr_features_1', arr_features_1)\n",
    "print(\"arr 1 saved\")\n",
    "arr_features_2 = get_features(list(tweets_2))\n",
    "np.save('./Pickles/arr_features_2', arr_features_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09144875-b7f6-45b8-92ed-047ccaf558fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine two batches of features\n",
    "arr_features_1 = np.load('./Pickles/arr_features_1.npy', allow_pickle=True)\n",
    "print(arr_features_1.shape)\n",
    "arr_features_2 = np.load('./Pickles/arr_features_2.npy', allow_pickle=True)\n",
    "print(arr_features_2.shape)\n",
    "arr_features = np.concatenate((arr_features_1, arr_features_2))\n",
    "np.save('./Pickles/arr_features2', arr_features)\n",
    "print(arr_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c723c87-5595-4fa3-ad95-2921b3ba7f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_features = np.load('./Pickles/arr_features.npy', allow_pickle=True)\n",
    "print(arr_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26aab7-8172-4e2e-835b-c9335730e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_features = arr_features[:5000]\n",
    "arr_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf5eaf-9603-4149-bbc3-726d84046209",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_features = arr_features[:5000]\n",
    "arr_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccc0ada-fcdd-4175-aa99-455ac919f37d",
   "metadata": {},
   "source": [
    "**Umap, clustering, and topic modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a515fd3-944c-4891-bf82-b8b63ad5d7f6",
   "metadata": {},
   "source": [
    "**Umap to high dimensions for clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91780e0a-943c-40f2-a8b3-0c78f300e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(len(arr_features), 4500)\n",
    "np.save('./Pickles/index', index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c2f23c-2c71-4361-9a9d-5f244964cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "index = np.load('./Pickles/index.npy', allow_pickle=True)\n",
    "\n",
    "obj = ParametricUMAP(n_neighbors=20,\n",
    "                            n_components=20,\n",
    "                            min_dist=0,\n",
    "                            metric='cosine',\n",
    "                            low_memory=True,\n",
    "                            verbose=True)\n",
    "obj.fit(arr_features[index])\n",
    "umap_highdim_embeddings = obj.transform(arr_features[index])\n",
    "np.save('./Pickles/umap_highdim_embeddings', umap_highdim_embeddings)\n",
    "print(umap_highdim_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03062278-6a19-4441-957a-1ed4ecc29934",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_highdim_embeddings = np.load('./Pickles/umap_highdim_embeddings.npy', allow_pickle=True)\n",
    "umap_highdim_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4da88-7b9c-49f6-ada7-f1560fe41d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Umap to low dimensions for plotting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656254f8-c36a-446f-b129-7ab0b3ca2e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "index = np.load('./Pickles/index.npy', allow_pickle=True)\n",
    "obj = ParametricUMAP(n_neighbors=20,\n",
    "                            n_components=2,\n",
    "                            min_dist=0.5,\n",
    "                            metric='cosine',\n",
    "                            low_memory=True,\n",
    "                            verbose=True)\n",
    "obj.fit(umap_highdim_embeddings)#index\n",
    "umap_embeddings = obj.transform(umap_highdim_embeddings)\n",
    "\n",
    "np.save('./Pickles/umap_embeddings', np.array(umap_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f73e19-1ed1-4666-9d2c-be582cd729a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "index = np.load('./Pickles/index.npy', allow_pickle=True)\n",
    "obj = ParametricUMAP(n_neighbors=20,\n",
    "                            n_components=3,\n",
    "                            min_dist=0.5,\n",
    "                            metric='cosine',\n",
    "                            low_memory=True,\n",
    "                            verbose=True)\n",
    "obj.fit(umap_highdim_embeddings)\n",
    "umap_embeddings_3d = obj.transform(umap_highdim_embeddings)\n",
    "\n",
    "np.save('./Pickles/umap_embeddings_3d', np.array(umap_embeddings_3d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90df9d6-8d1d-4854-8fe9-1b038247acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_trans = pd.read_pickle('./Pickles/df_trans.pkl')\n",
    "index = np.load('./Pickles/index.npy', allow_pickle=True)\n",
    "umap_embeddings = np.load('./Pickles/umap_embeddings.npy', allow_pickle=True)\n",
    "umap_embeddings_3d = np.load('./Pickles/umap_embeddings_3d.npy', allow_pickle=True)\n",
    "print(df_trans.shape)\n",
    "df_trans = df_trans.iloc[index]\n",
    "df_cluster = df_trans.assign(x=umap_embeddings[:, 0],y=umap_embeddings[:, 1], x3d=umap_embeddings_3d[:, 0], y3d=umap_embeddings_3d[:, 1], z3d=umap_embeddings_3d[:, 2])\n",
    "df_cluster.to_pickle('./Pickles/df_cluster.pkl')\n",
    "df_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3b4bc-aa10-4fc6-8aee-86650fdd2b50",
   "metadata": {},
   "source": [
    "**Hdbscan clustering to find topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d2935-e415-4422-a988-ba43ffe1a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_highdim_embeddings = np.load('./Pickles/umap_highdim_embeddings.npy', allow_pickle=True)\n",
    "df_cluster = pd.read_pickle('./Pickles/df_cluster.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb014cb-32be-4a83-bb9f-df36f02c79e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "cluster = hdbscan.HDBSCAN(#min_cluster_size=15,\n",
    "                          #min_samples=30,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom',\n",
    "                          #prediction_data=True,\n",
    "                         ).fit(umap_highdim_embeddings)\n",
    "df_cluster_topics = df_cluster.assign(topic=cluster.labels_)\n",
    "print(f\"{len(np.unique(cluster.labels_))} topics found\")\n",
    "df_cluster_topics\n",
    "df_cluster_topics.to_pickle('./Pickles/df_cluster_topics.pkl')\n",
    "#save model\n",
    "#with open('./Pickles/hdbscan.pkl', 'wb') as inp:\n",
    "#    dill.dump(cluster, inp)\n",
    "df_cluster_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8655c96-bf12-4b40-97e4-d708f291cf62",
   "metadata": {},
   "source": [
    "**TF-IDF to find important locally unique words per topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9f5379-e64d-4e02-bad5-997bf3fcb944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "df_per_topic = df_cluster_topics.groupby(['topic'], as_index = False).agg(({'text': ' '.join})) #create a dataframe organized by topic with all tweets in a topic concatenated\n",
    "stopwords_set = set(STOPWORDS)\n",
    "stopwords_set.update([\"user\", \"http\", \"located\", \"dtype\", \"actually\", \"quot\", 'text', '39', 'according', 'got']) # adding extra stop words\n",
    "stopwords_set=list(stopwords_set)\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords_set)\n",
    "X = tfidf.fit_transform(df_per_topic['text']) # fits  tfidf on entire corpus of tweets\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "def get_top_tf_idf_words(response, top_n=2): # function that returns top words given tweets in a specific topic\n",
    "    sorted_nzs = np.argsort(response.data)[:-(top_n+1):-1]\n",
    "    return feature_names[response.indices[sorted_nzs]]\n",
    "\n",
    "arr_tf_idf = []\n",
    "for i in np.arange(len(df_per_topic)):\n",
    "    responses = tfidf.transform([df_per_topic['text'][i]])\n",
    "    arr_tf_idf.append(list(get_top_tf_idf_words(responses,15))) # adds tf_idfs to per topic dataframe\n",
    "\n",
    "topic_tf_idf = dict(zip(np.arange(-1, len(df_per_topic)), arr_tf_idf))\n",
    "df_cluster_topics.sort_values(by='topic', inplace=True)\n",
    "topic_order = np.array(df_cluster_topics['topic'])\n",
    "df_cluster_topics['tf_idf'] = [topic_tf_idf[i] for i in topic_order]\n",
    "legend = dict(zip(np.arange(-1,len(df_per_topic)), [str(list(topic_tf_idf.keys())[i+1])+'_'+'_'.join(topic_tf_idf[i][0:5]) for i in np.arange(-1, len(topic_tf_idf) - 1)]))\n",
    "df_cluster_topics['legend'] = [legend[i] for i in topic_order]\n",
    "\n",
    "df_cluster_topics.to_pickle('./Pickles/df_cluster_topics.pkl')\n",
    "df_cluster_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f367721-23cd-4f50-80e0-6e3cebbd025a",
   "metadata": {},
   "source": [
    "**Interactive Scatter Plot with Plotly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c9899-132a-47bd-a81d-09c7b04844dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_topics = pd.read_pickle('./Pickles/df_cluster_topics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21247e23-6a32-44e2-a51e-4dc2395fc811",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusters_only = np.where(df_cluster_topics.topic != -1)\n",
    "#rand_idxs = np.random.choice(len(df_cluster_topics.iloc[clusters_only]), 20000, replace=False) # randomly pick 80000 points for file size purposes\n",
    "df_cluster_plotting = df_cluster_topics#.iloc[clusters_only].reset_index()#.iloc[rand_idxs]\n",
    "df_cluster_plotting.to_pickle('./Pickles/df_cluster_plotting.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb67d2b-ef26-4a00-84b2-27d05d3263d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_plotting = pd.read_pickle('./Pickles/df_cluster_plotting.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cbce09-4f84-4b51-829f-effb0695a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.scatter(\n",
    "    df_cluster_plotting,\n",
    "    x='x',\n",
    "    y='y',\n",
    "    custom_data=['text', 'tf_idf', 'topic'],\n",
    "    color='legend',#[str(i) for i in df_cluster_no_outlier.topic],\n",
    "    width=1600, height=1000,\n",
    "    template='seaborn',\n",
    ")\n",
    "# hover text style\n",
    "fig.update_layout(\n",
    "    hoverlabel=dict(\n",
    "        bgcolor=\"white\",\n",
    "        font_size=11,\n",
    "        font_family=\"Times New Roman\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# hover data\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"<br>\".join([\n",
    "        \"%{customdata[0]}\",\n",
    "        \"Topic %{customdata[2]} Keywords: %{customdata[1]}<extra></extra>\",\n",
    "    ])\n",
    ")\n",
    "\n",
    "# title\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"<b>Tweets by Topic 2D\"})\n",
    "\n",
    "# point size\n",
    "fig.update_traces(marker={'size': 2})\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"./Plots/tweet_by_topic_map.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c3c65-12d3-4f7c-9ac3-e4393c96371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d plot\n",
    "import plotly.express as px\n",
    "fig = px.scatter_3d(\n",
    "    df_cluster_plotting,\n",
    "    x='x3d',\n",
    "    y='y3d',\n",
    "    z='z3d',\n",
    "    custom_data=['text', 'tf_idf', 'topic'],\n",
    "    color='legend',\n",
    "    width=1600, height=1000,\n",
    "    template='seaborn',\n",
    ")\n",
    "# hover text style\n",
    "fig.update_layout(\n",
    "    hoverlabel=dict(\n",
    "        bgcolor=\"white\",\n",
    "        font_size=11,\n",
    "        font_family=\"Times New Roman\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# hover data\n",
    "fig.update_traces(\n",
    "    hovertemplate=\"<br>\".join([\n",
    "        \"%{customdata[0]}\",\n",
    "        \"Topic %{customdata[2]} Keywords: %{customdata[1]}<extra></extra>\",\n",
    "    ])\n",
    ")\n",
    "\n",
    "# title\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': \"<b>Tweets by Topic 3D\"})\n",
    "\n",
    "# point size\n",
    "fig.update_traces(marker={'size': 2})\n",
    "\n",
    "#fig.show();\n",
    "fig.write_html(\"./Plots/tweet_by_topic_map_3d.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eb19f6-61c2-453d-bedd-a2151137f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO\n",
    "\"\"\"\n",
    "Figure out how to best isolate pro ukraine and anti russia tweets.\n",
    "Remove html codes from text\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d710a-1419-4f71-bdcb-5112b732d248",
   "metadata": {},
   "source": [
    "**Once Topics are selected, map them over time per country**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e187bfe-7da5-43af-9517-b1a8ba0d22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [79, 13]\n",
    "languages = df_cluster_topics.language.unique()#['de', 'it', 'fr', 'es', 'pl', 'pt', 'el', 'fi', 'cs', 'ro']\n",
    "\n",
    "mask = df_cluster_topics.topic.isin(topics)\n",
    "\n",
    "df_tweet_bytopic_bylanguage = df_cluster_topics.groupby(['language', 'tweetcreatedts', 'topic']).size().unstack('language').reset_index().fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c4949-cafc-4872-b8f4-511319a21f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "#https://plotly.com/python/facet-plots/\n",
    "fig = go.Figure()\n",
    "fig = px.line(df_dailycounts, x='tweetcreatedts', y=df_dailycounts[final_languages].columns,\n",
    "              #hover_data={\"tweetcreatedts\": \"|%B %d, %Y\"},\n",
    "              template='seaborn',\n",
    "              title='Daily Tweets per language')\n",
    "fig.update_xaxes(\n",
    "    dtick=\"M1\",\n",
    "    tickformat=\"%b\\n%Y\")\n",
    "\n",
    "fig.show()\n",
    "fig.write_html(\"./Plots/.html\") # add button to normalize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
